import sys
import os
import json
import uuid
import random
import hashlib
import threading
import urllib.request
import zipfile
import tempfile
import time
from pathlib import Path
import traceback  # ç”¨äºæ›´è¯¦ç»†çš„é”™è¯¯è·Ÿè¸ª
import io  # ç”¨äºå†…å­˜æµå¤„ç†

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
OPTIONAL_DEPENDENCIES = {
    'ijson': 'æµå¼å¤„ç†å¤§å‹JSONæ–‡ä»¶',
    'tqdm': 'æä¾›è¿›åº¦æ¡æ˜¾ç¤º',
}

missing_dependencies = []
for package, description in OPTIONAL_DEPENDENCIES.items():
    try:
        __import__(package)
    except ImportError:
        missing_dependencies.append(f"{package} (ç”¨é€”: {description})")

# å°è¯•å¯¼å…¥PyQt5ï¼Œè‹¥ä¸å­˜åœ¨åˆ™æç¤ºå®‰è£…
try:
    from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, 
                               QLabel, QLineEdit, QPushButton, QProgressBar, QComboBox, 
                               QCheckBox, QTabWidget, QTextEdit, QFileDialog, QSpinBox,
                               QGroupBox, QRadioButton, QMessageBox, QScrollArea, QSizePolicy,
                               QStyleFactory, QFrame, QDialog, QTableWidget, QTableWidgetItem,
                               QHeaderView, QDialogButtonBox)
    from PyQt5.QtCore import Qt, QThread, pyqtSignal, QUrl, QSize, QTimer, QObject, QRunnable, QThreadPool
    from PyQt5.QtGui import QFont, QDesktopServices, QIcon, QColor, QPalette
except ImportError:
    print("PyQt5 æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install PyQt5")
    sys.exit(1)

# å°è¯•å¯¼å…¥å¿…è¦çš„ä¾èµ–
REQUIRED_DEPENDENCIES = {
    'datasets': 'load_dataset',
    'git': 'Repo',
    'requests': 'get',
}

for package, function in REQUIRED_DEPENDENCIES.items():
    try:
        module = __import__(package)
        # éªŒè¯æ¨¡å—æ˜¯å¦åŒ…å«é¢„æœŸçš„åŠŸèƒ½
        if function not in dir(module) and '.' not in function:
            print(f"è­¦å‘Š: {package} å·²å®‰è£…ï¼Œä½†å¯èƒ½ä¸æ˜¯æ­£ç¡®çš„ç‰ˆæœ¬")
    except ImportError:
        print(f"{package} æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install {package}")
        sys.exit(1)

# æˆåŠŸå¯¼å…¥æ‰€éœ€çš„æ¨¡å—
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨hf-mirror.comä½œä¸ºé•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"  # ç¦ç”¨ç¬¦å·é“¾æ¥è­¦å‘Š
os.environ["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = "1"    # ç¦ç”¨éšå¼ä»¤ç‰ŒéªŒè¯

from datasets import load_dataset
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import git
import requests

# å°è¯•å¯¼å…¥å¯é€‰æ¨¡å—ï¼Œç”¨äºæµå¼å¤„ç†å’Œè¿›åº¦æ¡
try:
    import ijson
    IJSON_AVAILABLE = True
except ImportError:
    IJSON_AVAILABLE = False

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# å¦‚æœæœ‰ç¼ºå¤±çš„å¯é€‰ä¾èµ–ï¼Œåœ¨å¯åŠ¨æ—¶æ˜¾ç¤ºä¸€æ¬¡è­¦å‘Š
if missing_dependencies:
    print("è­¦å‘Š: ä»¥ä¸‹å¯é€‰ä¾èµ–æœªå®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™:")
    for dep in missing_dependencies:
        print(f"  - {dep}")
    print("å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…å…¨éƒ¨å¯é€‰ä¾èµ–:")
    print(f"  pip install {' '.join([dep.split()[0] for dep in missing_dependencies])}")
    print("ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œä½†éƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™ã€‚")

class DownloadProgressTracker:
    """ç”¨äºè·Ÿè¸ªä¸‹è½½è¿›åº¦çš„ç±»"""
    def __init__(self, status_callback, total_size=None):
        self.status_callback = status_callback
        self.total_size = total_size
        self.current_size = 0
        self.last_report_time = 0
        self.start_time = time.time()

    def update(self, block_count, block_size, total_size):
        """æ›´æ–°ä¸‹è½½è¿›åº¦"""
        if self.total_size is None:
            self.total_size = total_size
        current_time = time.time()
        self.current_size = block_count * block_size
        # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹æ›´æ–°UI
        if current_time - self.last_report_time >= 0.5:
            percentage = min(100, int(self.current_size * 100 / self.total_size)) if self.total_size > 0 else 0
            elapsed_time = current_time - self.start_time
            speed = self.current_size / elapsed_time if elapsed_time > 0 else 0
            # æ ¼å¼åŒ–é€Ÿåº¦æ˜¾ç¤º
            if (speed < 1024):
                speed_str = f"{speed:.2f} B/s"
            elif speed < 1024 * 1024:
                speed_str = f"{speed/1024:.2f} KB/s"
            else:
                speed_str = f"{speed/(1024*1024):.2f} MB/s"
            # ä¼°è®¡å‰©ä½™æ—¶é—´
            if speed > 0 and self.total_size > 0:
                remaining_bytes = self.total_size - self.current_size
                remaining_time = remaining_bytes / speed
                if remaining_time < 60:
                    time_str = f"{remaining_time:.0f}ç§’"
                elif remaining_time < 3600:
                    time_str = f"{remaining_time/60:.1f}åˆ†é’Ÿ"
                else:
                    time_str = f"{remaining_time/3600:.1f}å°æ—¶"
                status = f"ä¸‹è½½è¿›åº¦: {percentage}% ({self.current_size/1024/1024:.1f}MB/{self.total_size/1024/1024:.1f}MB) - {speed_str} - å‰©ä½™æ—¶é—´: {time_str}"
            else:
                status = f"ä¸‹è½½è¿›åº¦: {percentage}% ({self.current_size/1024/1024:.1f}MB) - {speed_str}"
            self.status_callback(status)
            self.last_report_time = current_time

class DatasetWorker(QThread):
    """åå°å·¥ä½œçº¿ç¨‹ï¼Œç”¨äºå¤„ç†è€—æ—¶çš„æ•°æ®é›†æ“ä½œ"""
    progress_signal = pyqtSignal(int)
    status_signal = pyqtSignal(str)
    error_signal = pyqtSignal(str)
    finished_signal = pyqtSignal(bool, str)  # æˆåŠŸ/å¤±è´¥, æ¶ˆæ¯
    download_progress_signal = pyqtSignal(int, int)  # å½“å‰å¤§å°, æ€»å¤§å°

    def __init__(self, params):
        super().__init__()
        self.params = params
        self.is_running = True

    def stop(self):
        self.is_running = False
        self.wait(1000)  # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡æ­»

    def run(self):
        try:
            # æå–å‚æ•°
            source_type = self.params.get('source_type')
            dataset_name = self.params.get('dataset_name')
            dataset_url = self.params.get('dataset_url')
            github_repo = self.params.get('github_repo')
            output_formats = self.params.get('output_formats', {})
            output_dir = self.params.get('output_dir')
            filter_non_ascii = self.params.get('filter_non_ascii', True)
            random_seed = self.params.get('random_seed', 42)
            split_sizes = self.params.get('split_sizes', {'train': 0, 'val': 0, 'test': 0})
            # æ–°å¢: åˆ†å±‚æŠ½æ ·å‚æ•°
            use_stratified_sampling = self.params.get('use_stratified_sampling', False)
            subsets = self.params.get('subsets', {})

            self.status_signal.emit("åˆå§‹åŒ–æ•°æ®å¤„ç†æµç¨‹...")
            self.progress_signal.emit(1)

            # è®¾ç½®æ¯ä¸ªé˜¶æ®µçš„è¿›åº¦æ¯”ä¾‹
            progress_stages = {
                'load_data': (0, 30),      # åŠ è½½æ•°æ®é˜¶æ®µ: 0-30%
                'process_data': (30, 50),  # å¤„ç†æ•°æ®é˜¶æ®µ: 30-50%
                'split_data': (50, 70),    # æ‹†åˆ†æ•°æ®é˜¶æ®µ: 50-70%
                'save_data': (70, 90),     # ä¿å­˜æ•°æ®é˜¶æ®µ: 70-90%
                'verify_data': (90, 100)   # éªŒè¯æ•°æ®é˜¶æ®µ: 90-100%
            }

            self.status_signal.emit("å‡†å¤‡åŠ è½½æ•°æ®...")
            self.progress_signal.emit(progress_stages['load_data'][0])

            data = None
            if source_type == "huggingface":
                if use_stratified_sampling and subsets:
                    # ä½¿ç”¨åˆ†å±‚æŠ½æ ·æ—¶ï¼Œéœ€è¦åˆ†åˆ«åŠ è½½æ¯ä¸ªå­é›†
                    data = self._load_from_huggingface_with_subsets(dataset_name, subsets)
                else:
                    data = self._load_from_huggingface(dataset_name)
            elif source_type == "github":
                data = self._load_from_github(github_repo)
            elif source_type == "url":
                data = self._load_from_url(dataset_url)
            elif source_type == "local":
                data = self._load_from_local(self.params.get('local_file'))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}")

            if not self.is_running:
                return

            self.progress_signal.emit(progress_stages['process_data'][0])

            # å¤„ç†æ•°æ®
            # æ£€æŸ¥æ˜¯å¦åŠ è½½äº†æœ‰åˆ†çº§ç»“æ„çš„å­é›†
            if use_stratified_sampling and isinstance(data, dict) and 'subsets' in data and 'split_structure' in data:
                self.status_signal.emit("æ£€æµ‹åˆ°å¤šçº§å­é›†ç»“æ„...")
                subset_data = data['subsets']
                split_structure = data['split_structure']
                total_count = data['total_count']
                
                # é¢„å¤„ç†ï¼šæ ¹æ®åˆ†çº§ç»“æ„æ•´ç†æ•°æ®
                structured_data = {
                    'train': [],
                    'val': [],
                    'test': []
                }
                
                # å¤„ç†å„å­é›†æ•°æ®
                for subset_key, subset_items in subset_data.items():
                    if not self.is_running:
                        return
                    
                    # åˆ¤æ–­å­é›†æ˜¯å¦å±äºé¢„å®šä¹‰åˆ†çº§
                    target_split = None
                    if '/' in subset_key:
                        subset_name, split_name = subset_key.split('/')
                        if split_name == 'train':
                            target_split = 'train'
                        elif split_name in ('test'):
                            target_split = 'test'
                        elif split_name in ('val', 'validation'):
                            target_split = 'val'
                    
                    if target_split:
                        # ç›´æ¥æŒ‰å¯¹åº”åˆ†çº§å¤„ç†
                        self.status_signal.emit(f"å¤„ç†å­é›† '{subset_key}' ({len(subset_items)} æ¡è®°å½•) åˆ° {target_split}...")
                        
                        # éšæœºæ‰“ä¹±
                        random.seed(random_seed)
                        subset_items_list = list(subset_items)
                        random.shuffle(subset_items_list)
                        
                        # è¿‡æ»¤éASCIIå­—ç¬¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if filter_non_ascii:
                            filtered_items, filtered_count = self._filter_non_ascii(
                                subset_items_list, 
                                progress_stages['process_data'][0], 
                                progress_stages['process_data'][0] + (progress_stages['process_data'][1] - progress_stages['process_data'][0]) / len(subset_data)
                            )
                            self.status_signal.emit(f"å­é›† '{subset_key}' è¿‡æ»¤æ‰ {filtered_count}/{len(subset_items_list)} æ¡éASCIIè®°å½•")
                            structured_data[target_split].extend(filtered_items)
                        else:
                            structured_data[target_split].extend(subset_items_list)
                    else:
                        # æ²¡æœ‰æ˜ç¡®åˆ†çº§çš„å­é›†ï¼Œåç»­è¿›è¡Œåˆ†å±‚æŠ½æ ·
                        self.status_signal.emit(f"å­é›† '{subset_key}' æ²¡æœ‰æ˜ç¡®åˆ†çº§ï¼Œå°†æŒ‰éœ€åˆ†é…...")
                        
                        # éšæœºæ‰“ä¹±
                        random.seed(random_seed)
                        subset_items_list = list(subset_items)
                        random.shuffle(subset_items_list)
                        
                        # è¿‡æ»¤éASCIIå­—ç¬¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if filter_non_ascii:
                            filtered_items, filtered_count = self._filter_non_ascii(
                                subset_items_list,
                                progress_stages['process_data'][0],
                                progress_stages['process_data'][0] + (progress_stages['process_data'][1] - progress_stages['process_data'][0]) / len(subset_data)
                            )
                            self.status_signal.emit(f"å­é›† '{subset_key}' è¿‡æ»¤æ‰ {filtered_count}/{len(subset_items_list)} æ¡éASCIIè®°å½•")
                            # æš‚å­˜æœªåˆ†é…çš„æ•°æ®ï¼Œåç»­æ ¹æ®éœ€è¦åˆ†é…
                            structured_data.setdefault('unallocated', []).extend(filtered_items)
                        else:
                            structured_data.setdefault('unallocated', []).extend(subset_items_list)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†çš„å¤§å°è¦æ±‚
                required_sizes = {
                    'train': split_sizes.get('train', 0),
                    'val': split_sizes.get('val', 0),
                    'test': split_sizes.get('test', 0)
                }
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°æ®ä¸è¶³çš„æƒ…å†µ
                for split_name in ['train', 'val', 'test']:
                    if len(structured_data.get(split_name, [])) < required_sizes[split_name]:
                        shortage = required_sizes[split_name] - len(structured_data.get(split_name, []))
                        self.status_signal.emit(f"âš ï¸ {split_name} æ•°æ®é›†æ•°é‡ä¸è¶³ï¼Œç¼ºå°‘ {shortage} æ¡è®°å½•")
                        
                        # å¦‚æœæœ‰æœªåˆ†é…æ•°æ®ï¼Œä»ä¸­è¡¥å……
                        if 'unallocated' in structured_data and structured_data['unallocated']:
                            available = min(shortage, len(structured_data['unallocated']))
                            if available > 0:
                                self.status_signal.emit(f"ä»æœªåˆ†é…æ•°æ®ä¸­è¡¥å…… {available} æ¡è®°å½•åˆ° {split_name}")
                                structured_data.setdefault(split_name, []).extend(structured_data['unallocated'][:available])
                                structured_data['unallocated'] = structured_data['unallocated'][available:]
                
                # æœ€ç»ˆåˆ†é…æ•°æ®
                train_data = structured_data.get('train', [])[:required_sizes['train']] if required_sizes['train'] > 0 else []
                val_data = structured_data.get('val', [])[:required_sizes['val']] if required_sizes['val'] > 0 else []
                test_data = structured_data.get('test', [])[:required_sizes['test']] if required_sizes['test'] > 0 else []
                
                # å¦‚æœä»æœ‰æ•°æ®ä¸è¶³ï¼Œè¾“å‡ºè­¦å‘Š
                for split_name, data_list in zip(['è®­ç»ƒé›†', 'éªŒè¯é›†', 'æµ‹è¯•é›†'], [train_data, val_data, test_data]):
                    if len(data_list) < required_sizes[split_name.replace('è®­ç»ƒé›†', 'train').replace('éªŒè¯é›†', 'val').replace('æµ‹è¯•é›†', 'test')]:
                        self.status_signal.emit(f"âš ï¸ {split_name}æ•°æ®ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ•°æ®æºæˆ–å‡å°‘éœ€æ±‚é‡")
                
                self.status_signal.emit(f"æ•°æ®åˆ†é…å®Œæˆ: è®­ç»ƒé›†={len(train_data)}, éªŒè¯é›†={len(val_data)}, æµ‹è¯•é›†={len(test_data)}")
            # åŸæœ‰çš„å¤„ç†é€»è¾‘ä¿æŒä¸å˜
            elif use_stratified_sampling and isinstance(data, dict) and 'subsets' in data:
                self.status_signal.emit("ä½¿ç”¨åˆ†å±‚æŠ½æ ·å¤„ç†å¤šä¸ªå­é›†...")
                subset_data = data['subsets']
                total_count = data['total_count']
                self.status_signal.emit(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {total_count} æ¡è®°å½•ï¼Œ{len(subset_data)} ä¸ªå­é›†")

                # å¤„ç†å„å­é›†æ•°æ®
                processed_subset_data = {}
                for subset_name, subset_items in subset_data.items():
                    if not self.is_running:
                        return
                    self.status_signal.emit(f"å¤„ç†å­é›† '{subset_name}' ({len(subset_items)} æ¡è®°å½•)...")

                    # éšæœºæ‰“ä¹±
                    random.seed(random_seed)
                    subset_items_list = list(subset_items)
                    random.shuffle(subset_items_list)

                    # è¿‡æ»¤éASCIIå­—ç¬¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if filter_non_ascii:
                        filtered_items, filtered_count = self._filter_non_ascii(
                            subset_items_list, 
                            progress_stages['process_data'][0], 
                            progress_stages['process_data'][0] + (progress_stages['process_data'][1] - progress_stages['process_data'][0]) / len(subset_data)
                        )
                        self.status_signal.emit(f"å­é›† '{subset_name}' è¿‡æ»¤æ‰ {filtered_count}/{len(subset_items_list)} æ¡éASCIIè®°å½•")
                        processed_subset_data[subset_name] = filtered_items
                    else:
                        processed_subset_data[subset_name] = subset_items_list

                # æŒ‰æ¯”ä¾‹åˆ†å±‚æŠ½æ ·
                train_data, val_data, test_data = self._stratified_split(
                    processed_subset_data,
                    split_sizes['train'],
                    split_sizes['val'],
                    split_sizes['test']
                )
                self.progress_signal.emit(progress_stages['split_data'][1])
            else:
                # å¸¸è§„å¤„ç†: éšæœºæ‰“ä¹±æ•°æ®
                all_data = list(data)
                self.status_signal.emit(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•")
                self.status_signal.emit("æ­£åœ¨éšæœºæ‰“ä¹±æ•°æ®...")
                random.seed(random_seed)
                random.shuffle(all_data)
                self.progress_signal.emit(progress_stages['process_data'][0] + 5)

                # è¿‡æ»¤éASCIIå­—ç¬¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if filter_non_ascii:
                    self.status_signal.emit("æ­£åœ¨è¿‡æ»¤éASCIIå­—ç¬¦...")
                    filtered_data, filtered_count = self._filter_non_ascii(all_data, 
                                                                         progress_stages['process_data'][0] + 5, 
                                                                         progress_stages['process_data'][1])
                    self.status_signal.emit(f"è¿‡æ»¤äº† {filtered_count}/{len(all_data)} æ¡è®°å½• ({filtered_count/len(all_data)*100:.2f}%)")
                    all_data = filtered_data
                else:
                    self.progress_signal.emit(progress_stages['process_data'][1])

                # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
                total_needed = sum(split_sizes.values())
                if len(all_data) < total_needed:
                    self.status_signal.emit(f"è­¦å‘Š: å¯ç”¨æ•°æ®é‡({len(all_data)})å°‘äºè¦æ±‚æ•°é‡({total_needed})ï¼Œå°†æŒ‰æ¯”ä¾‹è°ƒæ•´...")
                    ratio = len(all_data) / total_needed
                    for key in split_sizes:
                        split_sizes[key] = int(split_sizes[key] * ratio)
                    split_sizes['test'] = min(split_sizes['test'], len(all_data) - split_sizes['train'] - split_sizes['val'])
                    self.status_signal.emit(f"è°ƒæ•´åçš„æ•°æ®é›†å¤§å°: è®­ç»ƒé›†={split_sizes['train']}, éªŒè¯é›†={split_sizes['val']}, æµ‹è¯•é›†={split_sizes['test']}")

                # åˆ’åˆ†æ•°æ®é›†
                self.status_signal.emit("æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
                self.progress_signal.emit(progress_stages['split_data'][0])

                train_size = split_sizes.get('train', 0)
                val_size = split_sizes.get('val', 0)
                test_size = split_sizes.get('test', 0)

                train_data = all_data[:train_size] if train_size > 0 else []
                val_data = all_data[train_size:train_size + val_size] if val_size > 0 else []
                test_data = all_data[train_size + val_size:train_size + val_size + test_size] if test_size > 0 else []

                self.progress_signal.emit(progress_stages['split_data'][0] + 10)

            # æ£€æŸ¥æ•°æ®äº¤å‰æ±¡æŸ“ï¼ˆç¡®ä¿æ²¡æœ‰é‡å¤æ•°æ®ï¼‰
            if len(train_data) > 0 or len(val_data) > 0 or len(test_data) > 0:
                self.status_signal.emit("æ£€æŸ¥æ•°æ®é›†é—´çš„äº¤å‰æ±¡æŸ“...")
                
                # ä½¿ç”¨é›†åˆè®°å½•å·²åˆ†é…çš„æ•°æ®å“ˆå¸Œï¼Œæ£€æµ‹é‡å¤
                all_data_hashes = set()
                duplicate_count = 0
                
                # æ£€æŸ¥è®­ç»ƒé›†
                if train_data:
                    clean_train_data = []
                    for item in train_data:
                        item_hash = hashlib.md5(str(item).encode()).hexdigest()
                        if item_hash not in all_data_hashes:
                            all_data_hashes.add(item_hash)
                            clean_train_data.append(item)
                        else:
                            duplicate_count += 1
                    
                    if duplicate_count > 0:
                        self.status_signal.emit(f"âš ï¸ ä»è®­ç»ƒé›†ç§»é™¤ {duplicate_count} æ¡é‡å¤è®°å½•")
                    train_data = clean_train_data
                
                # æ£€æŸ¥éªŒè¯é›†
                if val_data:
                    duplicate_count = 0
                    clean_val_data = []
                    for item in val_data:
                        item_hash = hashlib.md5(str(item).encode()).hexdigest()
                        if item_hash not in all_data_hashes:
                            all_data_hashes.add(item_hash)
                            clean_val_data.append(item)
                        else:
                            duplicate_count += 1
                    
                    if duplicate_count > 0:
                        self.status_signal.emit(f"âš ï¸ ä»éªŒè¯é›†ç§»é™¤ {duplicate_count} æ¡é‡å¤è®°å½•")
                    val_data = clean_val_data
                
                # æ£€æŸ¥æµ‹è¯•é›†
                if test_data:
                    duplicate_count = 0
                    clean_test_data = []
                    for item in test_data:
                        item_hash = hashlib.md5(str(item).encode()).hexdigest()
                        if item_hash not in all_data_hashes:
                            all_data_hashes.add(item_hash)
                            clean_test_data.append(item)
                        else:
                            duplicate_count += 1
                    
                    if duplicate_count > 0:
                        self.status_signal.emit(f"âš ï¸ ä»æµ‹è¯•é›†ç§»é™¤ {duplicate_count} æ¡é‡å¤è®°å½•")
                    test_data = clean_test_data
                
                self.status_signal.emit(f"âœ… æ•°æ®å»é‡å®Œæˆï¼Œç¡®ä¿æ•°æ®é›†é—´æ— äº¤å‰æ±¡æŸ“")
            
            self.progress_signal.emit(progress_stages['split_data'][1])

            # ä¿å­˜æ•°æ®é›†
            self.status_signal.emit("æ­£åœ¨ä¿å­˜æ•°æ®é›†...")
            saved_files = []

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)

            # è®¡ç®—ä¿å­˜è¿›åº¦å¢é‡
            save_progress_increment = (progress_stages['save_data'][1] - progress_stages['save_data'][0]) / 3  # 3ä¸ªæ•°æ®é›†
            current_progress = progress_stages['save_data'][0]

            # ä¿å­˜è®­ç»ƒé›†
            if train_data and len(train_data) > 0:
                self.status_signal.emit(f"æ­£åœ¨ä¿å­˜è®­ç»ƒé›† ({len(train_data)} æ¡è®°å½•)...")
                train_format = output_formats.get('train', 'json')
                train_filename = os.path.join(output_dir, f"train_{len(train_data)}.{train_format}")
                self._save_dataset(train_data, train_filename, 'train', train_format)
                saved_files.append(train_filename)
                current_progress += save_progress_increment
                self.progress_signal.emit(int(current_progress))

            # ä¿å­˜éªŒè¯é›†
            if val_data and len(val_data) > 0:
                self.status_signal.emit(f"æ­£åœ¨ä¿å­˜éªŒè¯é›† ({len(val_data)} æ¡è®°å½•)...")
                val_format = output_formats.get('val', 'jsonl')
                val_filename = os.path.join(output_dir, f"val_{len(val_data)}.{val_format}")
                self._save_dataset(val_data, val_filename, 'val', val_format)
                saved_files.append(val_filename)
                current_progress += save_progress_increment
                self.progress_signal.emit(int(current_progress))

            # ä¿å­˜æµ‹è¯•é›†
            if test_data and len(test_data) > 0:
                self.status_signal.emit(f"æ­£åœ¨ä¿å­˜æµ‹è¯•é›† ({len(test_data)} æ¡è®°å½•)...")
                test_format = output_formats.get('test', 'jsonl')
                test_filename = os.path.join(output_dir, f"test_{len(test_data)}.{test_format}")
                self._save_dataset(test_data, test_filename, 'test', test_format)
                saved_files.append(test_filename)
                current_progress += save_progress_increment
                self.progress_signal.emit(int(current_progress))

            self.progress_signal.emit(progress_stages['verify_data'][0])

            # éªŒè¯ä¿å­˜çš„æ•°æ®
            if saved_files:
                self.status_signal.emit("éªŒè¯ä¿å­˜çš„æ•°æ®é›†...")
                if self._verify_saved_datasets(saved_files, output_formats):
                    self.status_signal.emit(f"âœ… å¤„ç†å®Œæˆ! å·²ä¿å­˜ {len(saved_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
                    self.finished_signal.emit(True, f"å¤„ç†å®Œæˆã€‚å·²ä¿å­˜æ•°æ®é›†åˆ°: {output_dir}")
                else:
                    self.status_signal.emit("âš ï¸ éƒ¨åˆ†æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¿å­˜çš„æ•°æ®é›†")
                    self.finished_signal.emit(False, "å¤„ç†éƒ¨åˆ†å®Œæˆï¼Œä½†éªŒè¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—äº†è§£è¯¦æƒ…ã€‚")
            else:
                self.status_signal.emit("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
                self.finished_signal.emit(True, "å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        except Exception as e:
            import traceback
            self.error_signal.emit(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            self.error_signal.emit(traceback.format_exc())
            self.finished_signal.emit(False, f"å¤„ç†å¤±è´¥: {str(e)}")

    def _load_from_huggingface_with_subsets(self, dataset_name, subsets):
        """ä» Hugging Face åŠ è½½å¤šä¸ªå­é›†çš„æ•°æ®é›†ï¼Œæ”¯æŒå­é›†ä¸­çš„åˆ†çº§ç»“æ„å¦‚train/test/val"""
        self.status_signal.emit(f"ğŸ”„ æ­£åœ¨ä» Hugging Face é•œåƒç«™ç‚¹åŠ è½½æ•°æ®é›† '{dataset_name}' çš„å¤šä¸ªå­é›†...")
        result = {
            'subsets': {},
            'split_structure': {},  # æ–°å¢ï¼šè®°å½•å­é›†æ˜¯å¦æœ‰åˆ†çº§ç»“æ„
            'total_count': 0
        }

        # æ£€æŸ¥æ¯ä¸ªå­é›†æ˜¯å¦åŒ…å«é¢„å®šä¹‰çš„split
        predefined_splits = ['train', 'test', 'validation', 'val']
        
        for subset_name, subset_info in subsets.items():
            self.status_signal.emit(f"æ£€æŸ¥å­é›† '{subset_name}' çš„ç»“æ„...")
            
            # å°è¯•è·å–å­é›†çš„splitä¿¡æ¯
            try:
                # å…ˆæ£€æŸ¥å­é›†æ˜¯å¦æœ‰æ˜ç¡®çš„åˆ†çº§ç»“æ„
                available_splits = []
                for split in predefined_splits:
                    try:
                        # ä½¿ç”¨å°æ ·æœ¬åŠ è½½æµ‹è¯•æ˜¯å¦å­˜åœ¨è¯¥split
                        test_data = load_dataset(dataset_name, name=subset_name, split=split, streaming=True)
                        # å°è¯•è·å–ä¸€æ¡è®°å½•ï¼Œç¡®è®¤splitæœ‰æ•ˆ
                        next(iter(test_data))
                        available_splits.append(split)
                        self.status_signal.emit(f"âœ“ å­é›† '{subset_name}' åŒ…å« '{split}' åˆ†çº§")
                    except Exception:
                        continue
                
                if available_splits:
                    # å­é›†æœ‰æ˜ç¡®çš„åˆ†çº§ç»“æ„
                    result['split_structure'][subset_name] = available_splits
                    
                    # ä¸ºæ¯ä¸ªæœ‰æ•ˆçš„splitåŠ è½½æ•°æ®
                    for split in available_splits:
                        split_key = f"{subset_name}/{split}"
                        self.status_signal.emit(f"æ­£åœ¨åŠ è½½å­é›† '{subset_name}' çš„ '{split}' åˆ†çº§...")
                        try:
                            split_data = load_dataset(dataset_name, name=subset_name, split=split)
                            data_count = len(split_data)
                            self.status_signal.emit(f"âœ… å·²åŠ è½½ '{split_key}'ï¼ŒåŒ…å« {data_count} æ¡è®°å½•")
                            result['subsets'][split_key] = split_data
                            result['total_count'] += data_count
                        except Exception as e:
                            self.error_signal.emit(f"âŒ åŠ è½½ '{split_key}' æ—¶å‡ºé”™: {str(e)}")
                else:
                    # å­é›†æ²¡æœ‰æ˜ç¡®çš„åˆ†çº§ï¼Œä½¿ç”¨é»˜è®¤çš„trainåˆ†çº§
                    self.status_signal.emit(f"å­é›† '{subset_name}' æ²¡æœ‰æ˜ç¡®çš„åˆ†çº§ç»“æ„ï¼Œä½¿ç”¨é»˜è®¤split")
                    try:
                        subset_dataset = load_dataset(dataset_name, name=subset_name, split='train')
                        data_count = len(subset_dataset)
                        self.status_signal.emit(f"âœ… å·²åŠ è½½å­é›† '{subset_name}'ï¼ŒåŒ…å« {data_count} æ¡è®°å½•")
                        result['subsets'][subset_name] = subset_dataset
                        result['total_count'] += data_count
                    except Exception as e:
                        self.error_signal.emit(f"âŒ ä»åŠ è½½å­é›† '{subset_name}' æ—¶å‡ºé”™: {str(e)}")
            except Exception as e:
                self.error_signal.emit(f"âŒ æ£€æŸ¥å­é›† '{subset_name}' ç»“æ„æ—¶å‡ºé”™: {str(e)}")
                
        self.status_signal.emit(f"âœ… æ‰€æœ‰å­é›†åŠ è½½å®Œæˆï¼Œæ€»è®¡ {result['total_count']} æ¡è®°å½•")
        return result

    def _stratified_split(self, subset_data, train_size, val_size, test_size):
        """æŒ‰æ¯”ä¾‹è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œä¿æŒå„å­é›†çš„åŸå§‹æ¯”ä¾‹

        Args:
            subset_data: å­é›†æ•°æ®å­—å…¸ {subset_name: [items...], ...}
            train_size: è®­ç»ƒé›†ç›®æ ‡å¤§å°
            val_size: éªŒè¯é›†ç›®æ ‡å¤§å°
            test_size: æµ‹è¯•é›†ç›®æ ‡å¤§å°

        Returns:
            (train_data, val_data, test_data): ä¸‰ä¸ªæ•°æ®é›†åˆ—è¡¨
        """
        self.status_signal.emit("æ‰§è¡ŒæŒ‰æ¯”ä¾‹åˆ†å±‚æŠ½æ ·...")
        total_count = sum(len(items) for items in subset_data.values())
        total_target_size = train_size + val_size + test_size

        if total_count < total_target_size:
            self.status_signal.emit(f"âš ï¸ å¯ç”¨æ•°æ®é‡({total_count})å°‘äºç›®æ ‡æ•°é‡({total_target_size})ï¼Œå°†æŒ‰æ¯”ä¾‹ç¼©å‡")
            ratio = total_count / total_target_size
            train_size = int(train_size * ratio)
            val_size = int(val_size * ratio)
            test_size = int(test_size * ratio)
            self.status_signal.emit(f"è°ƒæ•´åç›®æ ‡å¤§å°: è®­ç»ƒé›†={train_size}, éªŒè¯é›†={val_size}, æµ‹è¯•é›†={test_size}")

        train_data = []
        val_data = []
        test_data = []

        # è®¡ç®—å„å­é›†åº”è´¡çŒ®çš„æ•°é‡
        subset_contributions = {}
        for subset_name, items in subset_data.items():
            # è®¡ç®—å æ¯”é‡
            ratio = len(items) / total_count
            
            # æŒ‰æ¯”ä¾‹åˆ†é…
            subset_train = max(1, int(train_size * ratio)) if train_size > 0 else 0
            subset_val = max(1, int(val_size * ratio)) if val_size > 0 else 0
            subset_test = max(1, int(test_size * ratio)) if test_size > 0 else 0

            total_needed = subset_train + subset_val + subset_test
            if total_needed > len(items):
                # æŒ‰æ¯”ä¾‹ç¼©å‡
                scale_ratio = len(items) / total_needed
                subset_train = int(subset_train * scale_ratio)
                subset_val = int(subset_val * scale_ratio)
                subset_test = min(len(items) - subset_train - subset_val, subset_test)

            subset_contributions[subset_name] = {
                'ratio': ratio,
                'train': subset_train,
                'val': subset_val,
                'test': subset_test
            }

        # è®°å½•æ¯ä¸ªå­é›†çš„è´¡çŒ®
        contribution_info = []
        for name, contribution in subset_contributions.items():
            subset_info = f"'{name}' (å æ¯” {contribution['ratio']:.1%}): è®­ç»ƒ={contribution['train']}, éªŒè¯={contribution['val']}, æµ‹è¯•={contribution['test']}"
            contribution_info.append(subset_info)
        self.status_signal.emit("å­é›†è´¡çŒ®åˆ†å¸ƒ:\n" + "\n".join(contribution_info))

        # ä»å„å­é›†æŒ‰è®¡ç®—å‡ºçš„æ•°é‡æŠ½å–æ ·æœ¬
        for subset_name, items in subset_data.items():
            contribution = subset_contributions[subset_name]
            current_items = items.copy()

            # æŠ½å–è®­ç»ƒé›†
            train_data.extend(current_items[:contribution['train']])
            current_pos = contribution['train']
            
            # æŠ½å–éªŒè¯é›†
            val_data.extend(current_items[current_pos:current_pos + contribution['val']])
            current_pos += contribution['val']

            # æŠ½å–æµ‹è¯•é›†
            test_data.extend(current_items[current_pos:current_pos + contribution['test']])

        # æœ€åå†æ¬¡æ‰“ä¹±å„æ•°æ®é›†ï¼Œé¿å…è¿ç»­å‡ºç°åŒä¸€å­é›†çš„æ•°æ®
        random.shuffle(train_data)
        random.shuffle(val_data)
        random.shuffle(test_data)

        self.status_signal.emit(f"åˆ†å±‚æŠ½æ ·å®Œæˆ: è®­ç»ƒé›†={len(train_data)}, éªŒè¯é›†={len(val_data)}, æµ‹è¯•é›†={len(test_data)}")
        return train_data, val_data, test_data

    def _load_from_huggingface(self, dataset_name):
        """ä» Hugging Face åŠ è½½æ•°æ®é›†"""
        self.status_signal.emit(f"ğŸ”„ æ­£åœ¨ä» Hugging Face é•œåƒç«™ç‚¹åŠ è½½æ•°æ®é›†: {dataset_name}...")
        try:
            dataset = load_dataset(dataset_name, split='train')
            self.status_signal.emit(f"âœ… æˆåŠŸä» Hugging Face é•œåƒç«™ç‚¹åŠ è½½æ•°æ®é›†: {dataset_name}")
            return dataset
        except Exception as e:
            self.error_signal.emit(f"âŒ æ— æ³•ä» ModelScope é•œåƒç«™ç‚¹åŠ è½½æ•°æ®é›†: {str(e)}")
            raise

    def _create_session_with_retry(self):
        """åˆ›å»ºå¸¦æœ‰é‡è¯•æœºåˆ¶çš„HTTPä¼šè¯"""
        session = requests.Session()
        retry = Retry(
            total=5,
            backoff_factor=0.3,
            status_forcelist=[500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        return session

    def _load_from_github(self, repo_url):
        """ä»GitHubåŠ è½½æ•°æ®é›†"""
        self.status_signal.emit(f"ğŸ”„ æ­£åœ¨ä»GitHubå…‹éš†ä»“åº“: {repo_url}...")
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        try:
            # å…‹éš†ä»“åº“
            git.Repo.clone_from(repo_url, temp_dir, depth=1)
            self.status_signal.emit("âœ… ä»“åº“å…‹éš†å®Œæˆ")

            # æŸ¥æ‰¾JSONæ–‡ä»¶
            data = []
            json_files = list(Path(temp_dir).glob('**/*.json')) + list(Path(temp_dir).glob('**/*.jsonl'))
            if not json_files:
                raise ValueError(f"åœ¨ä»“åº“ä¸­æœªæ‰¾åˆ°JSONæˆ–JSONLæ•°æ®æ–‡ä»¶")
            self.status_signal.emit(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSON/JSONLæ–‡ä»¶")

            # ä½¿ç”¨æœ€å¤§çš„JSONæ–‡ä»¶
            largest_file = max(json_files, key=lambda x: x.stat().st_size)
            file_size_mb = largest_file.stat().st_size / (1024 * 1024)
            self.status_signal.emit(f"é€‰æ‹©æœ€å¤§çš„æ•°æ®æ–‡ä»¶: {largest_file.name} ({file_size_mb:.2f}MB)")

            # è¯»å–æ•°æ®
            with open(largest_file, 'r', encoding='utf-8') as f:
                if largest_file.suffix == '.json':
                    data = json.load(f)
                    self.status_signal.emit(f"åŠ è½½JSONæ•°æ®, åŒ…å« {len(data)} æ¡è®°å½•")
                else:  # .jsonl
                    data = [json.loads(line) for line in f]
                    self.status_signal.emit(f"åŠ è½½JSONLæ•°æ®, åŒ…å« {len(data)} æ¡è®°å½•")
            return data
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            import shutil
            self.status_signal.emit("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            shutil.rmtree(temp_dir, ignore_errors=True)

    def _load_from_url(self, url):
        """ä»URLä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ï¼Œä½¿ç”¨æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨"""
        import time
        self.status_signal.emit(f"ğŸ”„ æ­£åœ¨ä»URLä¸‹è½½æ•°æ®: {url}...")

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        temp_file.close()

        try:
            # æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®
            try:
                session = self._create_session_with_retry()
                response = session.head(url, allow_redirects=True, timeout=10)

                # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                if (response.status_code >= 400):
                    error_message = f"URLä¸å¯è®¿é—®: HTTPé”™è¯¯ {response.status_code}"
                    if response.status_code == 404:
                        error_message = f"èµ„æºä¸å­˜åœ¨ (404): è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®"
                    elif response.status_code == 403:
                        error_message = f"è®¿é—®è¢«æ‹’ç» (403): å¯èƒ½éœ€è¦æƒé™æˆ–ç½‘ç«™ç¦æ­¢çˆ¬è™«"
                    elif response.status_code >= 500:
                        error_message = f"æœåŠ¡å™¨é”™è¯¯ ({response.status_code}): è¯·ç¨åå†è¯•"
                    self.error_signal.emit(f"âŒ {error_message}")
                    raise requests.RequestException(error_message)

                total_size = int(response.headers.get('content-length', 0))
            except requests.exceptions.ConnectionError:
                self.error_signal.emit("âŒ è¿æ¥é”™è¯¯: æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
                raise
            except requests.exceptions.Timeout:
                self.error_signal.emit("âŒ è¿æ¥è¶…æ—¶: æœåŠ¡å™¨å“åº”æ—¶é—´è¿‡é•¿ï¼Œè¯·ç¨åå†è¯•")
                raise
            except requests.RequestException as e:
                self.error_signal.emit(f"âŒ è¯·æ±‚é”™è¯¯: {str(e)}")
                raise

            # ä½¿ç”¨requestsä¸‹è½½å¹¶æ˜¾ç¤ºè¿›åº¦
            self.status_signal.emit("å¼€å§‹ä¸‹è½½æ–‡ä»¶...")
            start_time = time.time()
            downloaded_size = 0
            last_update_time = 0

            with session.get(url, stream=True) as r:
                r.raise_for_status()
                with open(temp_file.name, 'wb') as f:
                    # æ›´å¤§çš„å—å¤§å°ï¼Œæé«˜ä¸‹è½½é€Ÿåº¦ (8KB -> 64KB)
                    chunk_size = 65536  # 64KB
                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if not self.is_running:
                            raise InterruptedError("ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ")
                        if chunk:  # è¿‡æ»¤keep-aliveæ–°å—
                            f.write(chunk)
                            downloaded_size += len(chunk)

                            # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…é¢‘ç¹æ›´æ–°UIå¯¼è‡´å¡é¡¿
                            current_time = time.time()
                            if current_time - last_update_time > 0.5:
                                elapsed = current_time - start_time
                                speed = downloaded_size / elapsed if elapsed > 0 else 0
                                # æ ¼å¼åŒ–é€Ÿåº¦
                                if speed < 1024:
                                    speed_str = f"{speed:.1f} B/s"
                                elif speed < 1024 * 1024:
                                    speed_str = f"{speed/1024:.1f} KB/s"
                                else:
                                    speed_str = f"{speed/(1024*1024):.1f} MB/s"
                                
                                # è®¡ç®—å‰©ä½™æ—¶é—´
                                percent = (downloaded_size / total_size * 100) if total_size > 0 else 0
                                if total_size > 0 and speed > 0:
                                    remaining_bytes = total_size - downloaded_size
                                    estimated_time = remaining_bytes / speed
                                    if estimated_time < 60:
                                        time_str = f"{estimated_time:.1f}ç§’"
                                    elif estimated_time < 3600:
                                        time_str = f"{estimated_time/60:.1f}åˆ†é’Ÿ"
                                    else:
                                        time_str = f"{estimated_time/3600:.1f}å°æ—¶"
                                    progress_msg = f"ä¸‹è½½è¿›åº¦: {percent:.1f}% ({downloaded_size/(1024*1024):.1f}MB/{total_size/(1024*1024):.1f}MB) - {speed_str} - é¢„è®¡å‰©ä½™: {time_str}"
                                else:
                                    progress_msg = f"ä¸‹è½½è¿›åº¦: {percent:.1f}% ({downloaded_size/(1024*1024):.1f}MB) - {speed_str}"
                                
                                self.status_signal.emit(progress_msg)
                                self.download_progress_signal.emit(downloaded_size, total_size)
                                last_update_time = current_time

            self.status_signal.emit(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ ({downloaded_size/(1024*1024):.2f}MB)")

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶å¤„ç†
            data = []
            file_extension = os.path.splitext(url)[1].lower()

            # JSONLæ–‡ä»¶ - ä½¿ç”¨æµå¼å¤„ç†
            if file_extension == '.jsonl':
                self.status_signal.emit("é€è¡Œè§£æJSONLæ–‡ä»¶...")
                with open(temp_file.name, 'r', encoding='utf-8') as f:
                    line_count = 0
                    last_update_time = time.time()

                    for line in f:
                        if not line.strip():  # è·³è¿‡ç©ºè¡Œ
                            continue
                        try:
                            item = json.loads(line)
                            data.append(item)
                            line_count += 1
                            if line_count % 1000 == 0:
                                current_time = time.time()
                                if current_time - last_update_time >= 1.0:
                                    self.status_signal.emit(f"å·²è§£æ {line_count} æ¡è®°å½•...")
                                    last_update_time = current_time
                        except json.JSONDecodeError as e:
                            error_message = f"JSONè§£æé”™è¯¯ (è¡Œ {line_count+1}): {str(e)}"
                            self.error_signal.emit(error_message)
                            # ç»§ç»­å¤„ç†ï¼Œè·³è¿‡é”™è¯¯è¡Œ
                            continue
                self.status_signal.emit(f"âœ… JSONLæ–‡ä»¶è§£æå®Œæˆï¼ŒåŠ è½½äº† {line_count} æ¡è®°å½•")

            # JSONæ–‡ä»¶ - å¯¹å¤§æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†
            elif file_extension == '.json':
                file_size = os.path.getsize(temp_file.name)
                is_large_file = file_size > 100 * 1024 * 1024  # 100MB
                if is_large_file and IJSON_AVAILABLE:
                    self.status_signal.emit("æ£€æµ‹åˆ°å¤§å‹JSONæ–‡ä»¶ï¼Œä½¿ç”¨æµå¼è§£æ...")
                    with open(temp_file.name, 'rb') as f:
                        count = 0
                        last_update_time = time.time()
                        for item in ijson.items(f, 'item'):
                            data.append(item)
                            count += 1
                            current_time = time.time()
                            if count % 1000 == 0:
                                current_time = time.time()
                                if current_time - last_update_time >= 1.0:
                                    self.status_signal.emit(f"å·²è§£æ {count} æ¡è®°å½•...")
                                    last_update_time = current_time
                        self.status_signal.emit(f"âœ… è§£æå®Œæˆï¼ŒåŠ è½½äº† {count} æ¡è®°å½•")
                else:
                    self.status_signal.emit("è§£æJSONæ–‡ä»¶...")
                    try:
                        with open(temp_file.name, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            self.status_signal.emit(f"âœ… è§£æå®Œæˆï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
                    except UnicodeDecodeError:
                        self.status_signal.emit("UTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
                        for encoding in ['latin1', 'cp1252']:
                            try:
                                with open(temp_file.name, 'r', encoding=encoding) as f:
                                    data = json.load(f)
                                    self.status_signal.emit(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè§£æï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
                                    break
                            except Exception:
                                continue
                        else:
                            raise UnicodeDecodeError("æ— æ³•è§£ç JSONæ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ç¼–ç æ­£ç¡®")

            # ZIPæ–‡ä»¶
            elif file_extension == '.zip':
                self.status_signal.emit("è§£å‹ZIPæ–‡ä»¶...")
                temp_dir = tempfile.mkdtemp()
                try:
                    with zipfile.ZipFile(temp_file.name, 'r') as zip_ref:
                        zip_ref.extractall(temp_dir)
                    self.status_signal.emit("æŸ¥æ‰¾JSON/JSONLæ–‡ä»¶...")
                    json_files = list(Path(temp_dir).glob('**/*.json')) + list(Path(temp_dir).glob('**/*.jsonl'))
                    if not json_files:
                        error_message = "ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°JSONæˆ–JSONLæ•°æ®æ–‡ä»¶"
                        self.error_signal.emit(f"âŒ {error_message}")
                        raise FileNotFoundError(error_message)
                    json_files.sort(key=lambda x: x.stat().st_size, reverse=True)
                    largest_file = json_files[0]
                    file_size_mb = largest_file.stat().st_size / (1024 * 1024)
                    self.status_signal.emit(f"æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨æœ€å¤§çš„æ–‡ä»¶: {largest_file.name} ({file_size_mb:.2f}MB)")
                    if largest_file.suffix.lower() == '.jsonl':
                        line_count = 0
                        with open(largest_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                if line.strip():
                                    data.append(json.loads(line))
                                    line_count += 1
                        self.status_signal.emit(f"âœ… JSONLæ–‡ä»¶è§£æå®Œæˆï¼ŒåŠ è½½äº† {line_count} æ¡è®°å½•")
                    else:  # .json
                        is_large_json = file_size_mb > 100  # å¤§äº100MBè§†ä¸ºå¤§æ–‡ä»¶
                        if is_large_json and IJSON_AVAILABLE:
                            count = 0
                            with open(largest_file, 'rb') as f:
                                for item in ijson.items(f, 'item'):
                                    data.append(item)
                                    count += 1
                            self.status_signal.emit(f"âœ… å¤§å‹JSONæ–‡ä»¶è§£æå®Œæˆï¼ŒåŠ è½½äº† {count} æ¡è®°å½•")
                        else:
                            with open(largest_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                            self.status_signal.emit(f"âœ… JSONæ–‡ä»¶è§£æå®Œæˆï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
                finally:
                    import shutil
                    self.status_signal.emit("å·²æ¸…ç†ä¸´æ—¶è§£å‹æ–‡ä»¶")
                    shutil.rmtree(temp_dir, ignore_errors=True)

            # å…¶ä»–æ–‡ä»¶ç±»å‹
            else:
                supported_formats = ".json, .jsonl, .zip"
                error_message = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}ï¼Œç›®å‰ä»…æ”¯æŒ {supported_formats}"
                self.error_signal.emit(f"âŒ {error_message}")
                raise ValueError(error_message)

            return data
        except requests.RequestException as e:
            if "æœªèƒ½è§£æ" in str(e) or "getaddrinfo failed" in str(e):
                self.error_signal.emit("âŒ DNSè§£æå¤±è´¥: æ— æ³•è§£æåŸŸåï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            elif "ConnectTimeoutError" in str(e.__class__.__name__):
                self.error_signal.emit("âŒ è¿æ¥è¶…æ—¶: æœåŠ¡å™¨å“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åå†è¯•")
            elif "Max retries exceeded" in str(e):
                self.error_signal.emit("âŒ è¿æ¥å¤±è´¥: å¤šæ¬¡é‡è¯•åä»ç„¶æ— æ³•è¿æ¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–URL")
            else:
                self.error_signal.emit(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
            raise
        except json.JSONDecodeError as e:
            self.error_signal.emit(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
            raise
        except zipfile.BadZipFile:
            self.error_signal.emit("âŒ æ— æ•ˆçš„ZIPæ–‡ä»¶: æ–‡ä»¶æŸåæˆ–ä¸æ˜¯ZIPæ ¼å¼")
            raise
        except Exception as e:
            self.error_signal.emit(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            raise
        finally:
            try:
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)
            except Exception as e:
                self.error_signal.emit(f"è­¦å‘Š: æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _load_from_local(self, file_path):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œå¯¹å¤§æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†"""
        self.status_signal.emit(f"ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°æ–‡ä»¶: {file_path}...")
        data = []
        try:
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)
            self.status_signal.emit(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f}MB")

            # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§æ–‡ä»¶ (>100MB)
            is_large_file = file_size > 100 * 1024 * 1024
            if file_path.endswith('.json'):
                # å¯¹äºå¤§å‹JSONæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨ijsonè¿›è¡Œæµå¼è§£æ
                if is_large_file and IJSON_AVAILABLE:
                    self.status_signal.emit("æ£€æµ‹åˆ°å¤§å‹JSONæ–‡ä»¶ï¼Œä½¿ç”¨æµå¼è§£æ...")
                    with open(file_path, 'rb') as f:
                        generator = ijson.items(f, 'item')
                        count = 0
                        last_update_time = time.time()
                        for item in generator:
                            data.append(item)
                            count += 1
                            if count % 1000 == 0:
                                current_time = time.time()
                                if current_time - last_update_time >= 1.0:
                                    self.status_signal.emit(f"å·²å¤„ç† {count} æ¡è®°å½•...")
                                    last_update_time = current_time
                            if not self.is_running:
                                self.status_signal.emit("å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
                                return []
                    self.status_signal.emit(f"âœ… æµå¼è§£æå®Œæˆï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
                else:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.status_signal.emit("è§£æJSONæ–‡ä»¶...")
                        data = json.load(f)
                        self.status_signal.emit(f"âœ… è§£æå®Œæˆï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
            elif file_path.endswith('.jsonl'):
                self.status_signal.emit("é€è¡Œè§£æJSONLæ–‡ä»¶...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    count = 0
                    last_update_time = time.time()
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            item = json.loads(line)
                            data.append(item)
                            count += 1
                            if count % 1000 == 0:
                                current_time = time.time()
                                if current_time - last_update_time >= 1.0:
                                    progress_percentage = min(99, int(f.tell() / file_size * 100))
                                    self.status_signal.emit(f"å·²å¤„ç† {count} è¡Œ... ({progress_percentage}%)")
                                    last_update_time = current_time
                            if not self.is_running:
                                self.status_signal.emit("å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
                                return []
                        except json.JSONDecodeError as e:
                            line_number = count + 1
                            error_message = f"ç¬¬ {line_number} è¡ŒJSONè§£æé”™è¯¯: {str(e)}"
                            self.error_signal.emit(error_message)
                            continue
                self.status_signal.emit(f"âœ… è§£æå®Œæˆï¼ŒåŠ è½½äº† {len(data)} æ¡è®°å½•")
            else:
                supported_formats = ".json æˆ– .jsonl"
                error_message = f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {os.path.splitext(file_path)[1]}ï¼Œç›®å‰ä»…æ”¯æŒ {supported_formats}"
                self.error_signal.emit(f"âŒ {error_message}")
                raise ValueError(error_message)
        except MemoryError:
            error_message = "å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ•´ä¸ªæ–‡ä»¶ã€‚å»ºè®®ä½¿ç”¨æ›´å°çš„æ•°æ®é›†æˆ–å¢åŠ ç³»ç»Ÿå†…å­˜ã€‚"
            self.error_signal.emit(f"âŒ {error_message}")
            raise
        except ValueError as e:
            self.error_signal.emit(f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}")
            raise
        except Exception as e:
            self.error_signal.emit(f"âŒ åŠ è½½æœ¬åœ°æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
        return data

    def _filter_non_ascii(self, data, start_progress, end_progress):
        """è¿‡æ»¤åŒ…å«éASCIIå­—ç¬¦çš„æ•°æ®, å¹¶æ›´æ–°è¿›åº¦"""
        filtered_data = []
        filtered_count = 0
        total_items = len(data)

        self.status_signal.emit(f"å¼€å§‹è¿‡æ»¤éASCIIå­—ç¬¦ (å…± {total_items} æ¡è®°å½•)...")
        for i, item in enumerate(data):
            if not self.is_running:
                break

            # æ›´æ–°è¿›åº¦
            if i % 100 == 0 or i == total_items - 1:
                progress = start_progress + (end_progress - start_progress) * (i / total_items)
                self.progress_signal.emit(int(progress))
                if i % 1000 == 0:
                    self.status_signal.emit(f"è¿‡æ»¤è¿›åº¦: {i}/{total_items} ({i/total_items*100:.1f}%)")

            # æ£€æŸ¥æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µ
            is_ascii_item = True
            for key, value in item.items():
                if isinstance(value, str) and not self._is_ascii(value):
                    is_ascii_item = False
                    break

            if is_ascii_item:
                filtered_data.append(item)
            else:
                filtered_count += 1

        self.progress_signal.emit(end_progress)
        return filtered_data, filtered_count

    def _is_ascii(self, s):
        """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åªåŒ…å«ASCIIå­—ç¬¦"""
        if s is None:
            return True
        return all(ord(c) < 128 for c in s)

    def _check_dataset_overlap(self, sets, name_mapping):
        """æ£€æŸ¥æ•°æ®é›†ä¹‹é—´æ˜¯å¦æœ‰é‡å """
        problem_hashes = {}

        for set_name, dataset in sets.items():
            if not dataset:
                continue

            self.status_signal.emit(f"æ£€æŸ¥ {name_mapping[set_name]} çš„å”¯ä¸€æ€§...")
            for item in dataset:
                item_hash = hashlib.md5(str(item).encode()).hexdigest()
                if item_hash in problem_hashes and problem_hashes[item_hash] != set_name:
                    self.status_signal.emit(f"âš ï¸ å‘ç°é‡å : {name_mapping[set_name]} å’Œ {name_mapping[problem_hashes[item_hash]]} ä¹‹é—´å­˜åœ¨ç›¸åŒå†…å®¹")
                    return False
                else:
                    problem_hashes[item_hash] = set_name

        return True

    def _save_dataset(self, data, filename, split_type, format_type):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                if format_type.lower() == 'json':
                    # è½¬æ¢æ•°æ®æ ¼å¼
                    converted_data = [self._convert_to_target_format(item, split_type == 'test') for item in data]
                    json.dump(converted_data, f, ensure_ascii=False, indent=4)
                else:  # jsonl
                    for item in data:
                        # è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼
                        converted_item = self._convert_to_target_format(item, split_type == 'test')
                        f.write(json.dumps(converted_item, ensure_ascii=False) + '\n')
            return True
        except Exception as e:
            self.error_signal.emit(f"âŒ ä¿å­˜æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
            return False

    def _convert_to_target_format(self, item, is_test=False):
        """
        å°†åŸå§‹æ•°æ®é¡¹è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼:
        {
            "instruction": "é—®é¢˜æˆ–æŒ‡ç¤º",
            "input": "",
            "output": "ç­”æ¡ˆ"
        }
        """
        # å°è¯•ä»ä¸åŒå¯èƒ½çš„å­—æ®µåä¸­è·å–å€¼
        instruction = ""
        output = ""
        input_text = ""
        
        # è·å–instructionå­—æ®µ (å¯èƒ½æ˜¯questionã€promptç­‰)
        for field in ['question', 'instruction', 'prompt', 'task', 'problem', 
                      'Task Description', 'Guide', 'Prompt']:
            if field in item:
                instruction = str(item[field])
                break
        
        # è·å–outputå­—æ®µ (å¯èƒ½æ˜¯answerã€responseç­‰)
        for field in ['answer', 'output', 'response', 'result', 'solution',
                     'Result', 'Outcome', 'Generated Output', 'Output Data']:
            if field in item:
                output = str(item[field]) if item[field] is not None else ""
                break
        
        # è·å–inputå­—æ®µ
        for field in ['input', 'context', 'data', 
                     'Data Input', 'Source Data', 'Input Variable', 'Feed-in']:
            if field in item:
                input_text = str(item[field]) if item[field] is not None else ""
                break
        
        # å¦‚æœæ˜¯æµ‹è¯•é›†ä¸”ç­”æ¡ˆä¸ºç©ºæˆ–NULLï¼Œç¡®ä¿ä¸ºç©ºå­—ç¬¦ä¸²
        if is_test and (output is None or output.lower() == "null"):
            output = ""
        
        # å¦‚æœä¸Šé¢çš„å­—æ®µåŒ¹é…å¤±è´¥ï¼Œåˆ™ä½¿ç”¨itemçš„ç¬¬ä¸€ä¸ªå­—æ®µä½œä¸ºinstruction
        if not instruction and item:
            instruction = str(next(iter(item.values())))
        
        return {
            "instruction": instruction,
            "input": input_text,
            "output": output
        }

    def _verify_saved_datasets(self, file_paths, output_formats):
        """éªŒè¯ä¿å­˜çš„æ•°æ®é›†æ–‡ä»¶"""
        for path in file_paths:
            try:
                if path.endswith('.json'):
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        self.status_signal.emit(f"âœ… éªŒè¯æˆåŠŸ: {path} ({len(data)} æ¡è®°å½•)")
                elif path.endswith('.jsonl'):
                    count = 0
                    with open(path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                json.loads(line)
                                count += 1
                    self.status_signal.emit(f"âœ… éªŒè¯æˆåŠŸ: {path} ({count} æ¡è®°å½•)")
            except Exception as e:
                self.error_signal.emit(f"âŒ éªŒè¯å¤±è´¥: {path} - {str(e)}")
                return False
        return True

# å–æ¶ˆå›¾å½¢åŒ–UIè®¾è®¡ï¼Œç»Ÿä¸€åœ¨ä»£ç å¼€å¤´è®¾ç½®å‚æ•°
import os
import random
from modelscope.msdatasets import MsDataset

"""
********************************************************************************
********************************************************************************
********************************************************************************
"""
# é…ç½®å‚æ•°
DATASET_NAME = "dahaizei/cnn_dailymail"  # æ•°æ®é›†åç§°
SUBSET_NAME = "2.0.0" # å­é›†åç§°
OUTPUT_DIR = f"./output/{DATASET_NAME}/2.0.0"  # è¾“å‡ºç›®å½•
FILE_FORMAT = "json"  # æ–‡ä»¶æ ¼å¼
SPLIT_SIZES = {"train": 3000, "val": 0, "test": 200}  # æ•°æ®é›†åˆ’åˆ†å¤§å°
FILTER_NON_ASCII = True  # æ˜¯å¦è¿‡æ»¤éASCIIå­—ç¬¦
RANDOM_SEED = 42  # éšæœºç§å­
"""
********************************************************************************
********************************************************************************
********************************************************************************
"""

# è®¾ç½®éšæœºç§å­
random.seed(RANDOM_SEED)

# åŠ è½½æ•°æ®é›†
print(f"ğŸ”„ æ­£åœ¨ä» ModelScope åŠ è½½æ•°æ®é›†: {DATASET_NAME}...")
try:
    all_data = []
    total_records = 0
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šå­é›†
    if SUBSET_NAME is None:
        # ç›´æ¥åŠ è½½æ•´ä¸ªæ•°æ®é›†
        print(f"ğŸ”„ æœªæŒ‡å®šå­é›†ï¼ŒåŠ è½½å®Œæ•´æ•°æ®é›†...")
        
        # å°è¯•åŠ è½½å®Œæ•´æ•°æ®é›†ï¼Œè‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„åˆ†å‰²
        available_splits = []
        for split_name in ["train", "test", "val", "validation", "dev"]:
            try:
                # å°è¯•åŠ è½½æ­¤åˆ†å‰²ä»¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                test_load = MsDataset.load(DATASET_NAME, split=split_name)
                available_splits.append(split_name)
                print(f"âœ“ æ•°æ®é›†åŒ…å«åˆ†å‰²: {split_name}")
            except Exception:
                pass
        
        if not available_splits:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢„å®šä¹‰åˆ†å‰²ï¼Œå°è¯•ä¸æŒ‡å®šsplitå‚æ•°
            try:
                dataset = MsDataset.load(DATASET_NAME)
                total_records = len(dataset)
                all_data = list(dataset)  # è½¬æ¢ä¸ºåˆ—è¡¨
                print(f"âœ… å®Œæ•´æ•°æ®é›†åŠ è½½æˆåŠŸï¼ˆæ— åˆ†å‰²ï¼‰ï¼Œå…± {total_records} æ¡è®°å½•")
            except Exception as e:
                print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
                exit(1)
        else:
            # ä¼˜å…ˆä½¿ç”¨trainåˆ†å‰²ï¼Œç„¶åæ˜¯å…¶ä»–åˆ†å‰²
            for preferred_split in ["train", "test", "val", "validation", "dev"]:
                if preferred_split in available_splits:
                    try:
                        print(f"ğŸ”„ åŠ è½½ '{preferred_split}' åˆ†å‰²...")
                        dataset = MsDataset.load(DATASET_NAME, split=preferred_split)
                        total_records = len(dataset)
                        all_data = list(dataset)  # è½¬æ¢ä¸ºåˆ—è¡¨
                        print(f"âœ… åˆ†å‰² '{preferred_split}' åŠ è½½æˆåŠŸï¼Œå…± {total_records} æ¡è®°å½•")
                        break
                    except Exception as e:
                        print(f"âš ï¸ å°è¯•åŠ è½½ '{preferred_split}' åˆ†å‰²æ—¶å‡ºé”™: {str(e)}")
                        continue
    else:
        # å¤„ç†æŒ‡å®šçš„å­é›†
        subset_names = SUBSET_NAME.split(',')
        
        for subset in subset_names:
            subset_name = subset.strip()
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½å­é›†: {subset_name}...")
            
            # æ£€æŸ¥å­é›†æœ‰å“ªäº›å¯ç”¨çš„åˆ†å‰²
            available_subset_splits = []
            for split_name in ["train", "test", "val", "validation", "dev"]:
                try:
                    # å°è¯•åŠ è½½æ­¤åˆ†å‰²ä»¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    test_load = MsDataset.load(DATASET_NAME, subset_name=subset_name, split=split_name)
                    available_subset_splits.append(split_name)
                    print(f"âœ“ å­é›† '{subset_name}' åŒ…å«åˆ†å‰²: {split_name}")
                except Exception:
                    pass
            
            if not available_subset_splits:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢„å®šä¹‰åˆ†å‰²ï¼Œå°è¯•ä¸æŒ‡å®šsplitå‚æ•°
                try:
                    subset_data = MsDataset.load(DATASET_NAME, subset_name=subset_name)
                    subset_data_list = list(subset_data)  # è½¬æ¢ä¸ºåˆ—è¡¨
                    print(f"âœ… å­é›† '{subset_name}' åŠ è½½æˆåŠŸï¼ˆæ— åˆ†å‰²ï¼‰ï¼Œå…± {len(subset_data_list)} æ¡è®°å½•")
                    all_data.extend(subset_data_list)
                    total_records += len(subset_data_list)
                except Exception as e:
                    print(f"âŒ å­é›† '{subset_name}' åŠ è½½å¤±è´¥: {str(e)}")
            else:
                # ä¼˜å…ˆä½¿ç”¨trainåˆ†å‰²ï¼Œç„¶åæ˜¯å…¶ä»–åˆ†å‰²
                loaded = False
                for preferred_split in ["train", "test", "val", "validation", "dev"]:
                    if preferred_split in available_subset_splits:
                        try:
                            print(f"ğŸ”„ åŠ è½½å­é›† '{subset_name}' çš„ '{preferred_split}' åˆ†å‰²...")
                            subset_data = MsDataset.load(DATASET_NAME, subset_name=subset_name, split=preferred_split)
                            subset_data_list = list(subset_data)  # è½¬æ¢ä¸ºåˆ—è¡¨
                            print(f"âœ… å­é›† '{subset_name}' çš„ '{preferred_split}' åˆ†å‰²åŠ è½½æˆåŠŸï¼Œå…± {len(subset_data_list)} æ¡è®°å½•")
                            all_data.extend(subset_data_list)
                            total_records += len(subset_data_list)
                            loaded = True
                            break
                        except Exception as e:
                            print(f"âš ï¸ å°è¯•åŠ è½½å­é›† '{subset_name}' çš„ '{preferred_split}' åˆ†å‰²æ—¶å‡ºé”™: {str(e)}")
                            continue
                
                if not loaded:
                    print(f"âŒ æ— æ³•åŠ è½½å­é›† '{subset_name}' çš„ä»»ä½•åˆ†å‰²")

        if total_records == 0:
            print("âŒ æ‰€æœ‰å­é›†åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†åç§°æˆ–å­é›†åç§°æ˜¯å¦æ­£ç¡®")
            exit(1)
            
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {total_records} æ¡è®°å½•")
except Exception as e:
    print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
    exit(1)

# éšæœºæ‰“ä¹±æ•°æ® - ç°åœ¨all_dataå·²ç»æ˜¯åˆ—è¡¨ï¼Œå¯ä»¥æ­£å¸¸æ‰“ä¹±
print("ğŸ”„ æ­£åœ¨éšæœºæ‰“ä¹±æ•°æ®...")
random.shuffle(all_data)

# è¿‡æ»¤éASCIIå­—ç¬¦
if FILTER_NON_ASCII:
    print("ğŸ”„ æ­£åœ¨è¿‡æ»¤éASCIIå­—ç¬¦...")
    filtered_data = []
    for item in all_data:
        if all(ord(c) < 128 for c in str(item)):
            filtered_data.append(item)
    print(f"âœ… è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(filtered_data)} æ¡è®°å½•")
    all_data = filtered_data

# æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
if len(all_data) < sum(SPLIT_SIZES.values()):
    print("WARING! æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æŒ‰æŒ‡å®šæ¯”ä¾‹åˆ’åˆ†.å¯èƒ½ä¼šå‡ºç°åˆ’åˆ†ä¸æ»¡çš„æƒ…å†µ")

# åˆ’åˆ†æ•°æ®é›†
print("ğŸ”„ æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
train_data = all_data[:SPLIT_SIZES["train"]]
val_data = all_data[SPLIT_SIZES["train"]:SPLIT_SIZES["train"] + SPLIT_SIZES["val"]]
test_data = all_data[SPLIT_SIZES["train"] + SPLIT_SIZES["val"]:SPLIT_SIZES["train"] + SPLIT_SIZES["val"] + SPLIT_SIZES["test"]]
print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›†={len(train_data)}, éªŒè¯é›†={len(val_data)}, æµ‹è¯•é›†={len(test_data)}")

# ä¿å­˜æ•°æ®é›†
for split_name, split_data in zip(["train", "val", "test"], [train_data, val_data, test_data]):
    if len(split_data) == 0:
        print(f"âš ï¸ {split_name} æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        continue

    # åˆ›å»ºæ–‡ä»¶å¤¹
    split_dir = os.path.join(OUTPUT_DIR, split_name)
    os.makedirs(split_dir, exist_ok=True)

    # å®šä¹‰æ–‡ä»¶å
    file_suffix = "dataset" if split_name == "train" else "val" if split_name == "val" else "benchmark"
    output_path = os.path.join(split_dir, f"{split_name}_{file_suffix}_{len(split_data)}.{FILE_FORMAT}")

    print(f"ğŸ”„ æ­£åœ¨ä¿å­˜ {split_name} æ•°æ®é›†åˆ° {output_path}...")

    # è½¬æ¢æ•°æ®æ ¼å¼
    def convert_to_target_format(item, is_test=False):
        """å°†åŸå§‹æ•°æ®é¡¹è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼"""
        instruction = ""
        output = ""
        input_text = ""
        
        # è·å–instructionå­—æ®µ
        for field in ['question', 'instruction', 'prompt', 'task', 'problem', 
                      'Task Description', 'Guide', 'Prompt']:
            if field in item:
                instruction = str(item[field])
                break
        
        # è·å–outputå­—æ®µ
        for field in ['answer', 'output', 'response', 'result', 'solution',
                     'Result', 'Outcome', 'Generated Output', 'Output Data']:
            if field in item:
                output = str(item[field]) if item[field] is not None else ""
                break
        
        # è·å–inputå­—æ®µ
        for field in ['input', 'context', 'data', 
                     'Data Input', 'Source Data', 'Input Variable', 'Feed-in']:
            if field in item:
                input_text = str(item[field]) if item[field] is not None else ""
                break
        
        # å¦‚æœæ˜¯æµ‹è¯•é›†ä¸”ç­”æ¡ˆä¸ºç©ºæˆ–NULLï¼Œç¡®ä¿ä¸ºç©ºå­—ç¬¦ä¸²
        if is_test and (output is None or output.lower() == "null"):
            output = ""
        
        # å¦‚æœä¸Šé¢çš„å­—æ®µåŒ¹é…å¤±è´¥ï¼Œåˆ™ä½¿ç”¨itemçš„ç¬¬ä¸€ä¸ªå€¼ä½œä¸ºinstruction
        if not instruction and item:
            instruction = str(next(iter(item.values())))
        
        return {
            "instruction": instruction,
            "input": input_text,
            "output": output
        }

    # è½¬æ¢æ‰€æœ‰æ•°æ®
    is_test = (split_name == "test")
    converted_data = [convert_to_target_format(item, is_test) for item in split_data]
    
    # ä¿å­˜ä¸ºJSONæˆ–JSONL
    with open(output_path, "w", encoding="utf-8") as f:
        if FILE_FORMAT.lower() == "json":
            # æ•´ä¸ªæ•°ç»„ä¿å­˜ä¸ºä¸€ä¸ªJSON
            json.dump(converted_data, f, ensure_ascii=False, indent=4)
        else:
            # æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
            for item in converted_data:
                json_line = json.dumps(item, ensure_ascii=False)
                f.write(f"{json_line}\n")
    
    print(f"âœ… {split_name} æ•°æ®é›†ä¿å­˜å®Œæˆ")

print("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ")
